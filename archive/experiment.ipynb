{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBanditEnv(gym.Env):\n",
    "    def __init__(self, num_agents=10, history_length = 5):\n",
    "        super(MultiArmedBanditEnv, self).__init__()\n",
    "        self.num_agents = num_agents\n",
    "        self.history_length = history_length\n",
    "        self.action_space = spaces.Discrete(2)  # 0: Not Cooperate, 1: Cooperate\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(history_length * num_agents,), dtype=np.float32)\n",
    "        \n",
    "        self.state = np.zeros(self.num_agents)\n",
    "        self.history = np.zeros((history_length, num_agents))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.rand(self.num_agents)\n",
    "         # initialize history with the current state repeated\n",
    "        self.history = np.tile(self.state, (self.history_length, 1))\n",
    "        return self.history.flatten()\n",
    "        # return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        rewards = np.zeros(self.num_agents)\n",
    "        new_state = np.zeros(self.num_agents)\n",
    "        \n",
    "        # iterate through pairs of agents to evaluate their actions\n",
    "        for i in range(0, self.num_agents, 2):\n",
    "            action1 = actions[i]\n",
    "            action2 = actions[i+1]\n",
    "\n",
    "            if action1 == 1 and action2 == 1: #if  both cooperate\n",
    "                rewards[i] = rewards[i+1] = 5\n",
    "            elif action1 == 0 and action2 == 0: # if neither cooperate\n",
    "                rewards[i] = rewards[i+1] = 1\n",
    "            elif action1 == 1 and action2 == 0:\n",
    "                rewards[i] = 0\n",
    "                rewards[i+1] = 10\n",
    "            else:\n",
    "                rewards[i] = 10\n",
    "                rewards[i+1] = 0\n",
    "            \n",
    "            new_state[i] = action1\n",
    "            new_state[i+1] = action2\n",
    "\n",
    "        # self.state = np.random.rand(self.num_agents)\n",
    "        # update history\n",
    "        self.history = np.roll(self.history, -1, axis=0)\n",
    "        self.history[-1, :] = actions\n",
    "        done = True  # One-step game\n",
    "        return self.history.flatten(), rewards, done, {}\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, color, cooperation_probability):\n",
    "        self.color = color\n",
    "        self.cooperation_probability = cooperation_probability\n",
    "        self.rewards = []\n",
    "\n",
    "    def decide(self):\n",
    "        return np.random.rand() < self.cooperation_probability\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, num_agents, red_coop_prob, blue_coop_prob, history_length=10, learning_rate=0.1, discount_factor=0.95, exploration_rate=1.0, exploration_decay=0.99, min_epsilon=0.1):\n",
    "        self.num_agents = num_agents\n",
    "        self.history_length = history_length\n",
    "        self.agents = self._initialize_agents(red_coop_prob, blue_coop_prob)\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = exploration_rate\n",
    "        self.epsilon_decay = exploration_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "                # Limit history length to 10\n",
    "        if self.history_length > 10:\n",
    "            raise ValueError(\"History length must be 10 or less to manage Q-table size.\")\n",
    "\n",
    "        # Calculate number of states and actions with limited history\n",
    "        self.num_states = 2 ** (self.num_agents * self.history_length)\n",
    "        self.num_actions = 2 ** self.num_agents\n",
    "        \n",
    "        print(f\"num_states: {self.num_states}, num_actions: {self.num_actions}\")\n",
    "        \n",
    "        self.q_table = np.zeros((self.num_states, self.num_actions))\n",
    "\n",
    "    def _initialize_agents(self, red_coop_prob, blue_coop_prob):\n",
    "        agents = []\n",
    "        for i in range(self.num_agents):\n",
    "            if i < self.num_agents // 2:\n",
    "                agents.append(Agent('red', red_coop_prob))\n",
    "            else:\n",
    "                agents.append(Agent('blue', blue_coop_prob))\n",
    "        return agents\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, 2, size=self.num_agents)\n",
    "        else:\n",
    "            state_idx = self._state_to_index(state)\n",
    "            action_idx = np.argmax(self.q_table[state_idx])\n",
    "            return self._index_to_actions(action_idx)\n",
    "\n",
    "    def update_q_table(self, state, actions, reward, next_state):\n",
    "        state_idx = self._state_to_index(state)\n",
    "        next_state_idx = self._state_to_index(next_state)\n",
    "        action_idx = self._actions_to_index(actions)\n",
    "        \n",
    "        # normalize reward to improve stability\n",
    "        normalized_reward = (reward - reward.mean()) / (reward.std() + 1e-8)\n",
    "        \n",
    "        q_target = normalized_reward + self.gamma * np.max(self.q_table[next_state_idx])\n",
    "        self.q_table[state_idx, action_idx] += self.lr * (q_target - self.q_table[state_idx, action_idx])\n",
    "        # q_predict = self.q_table[state_idx, action_idx]\n",
    "        # q_target = reward + self.gamma * np.max(self.q_table[next_state_idx])\n",
    "        # self.q_table[state_idx, action_idx] += self.lr * (q_target)\n",
    "\n",
    "    def _state_to_index(self, state):\n",
    "        return int(\"\".join(map(str, (state >= 0.5).astype(int))), 2)\n",
    "\n",
    "    def _actions_to_index(self, actions):\n",
    "        return int(\"\".join(map(str, actions)), 2)\n",
    "\n",
    "    def _index_to_actions(self, index):\n",
    "        return [int(x) for x in np.binary_repr(index, width=self.num_agents)]\n",
    "\n",
    "    # reduces the exploration rate over time; shift from exploration to exploitation\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "        # self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_with_qlearning(cooperation_strategies, num_agents=10, num_episodes=1000, num_trials=10, history_length=5):\n",
    "    for red_coop, blue_coop in cooperation_strategies:\n",
    "        env = MultiArmedBanditEnv(num_agents=num_agents, history_length=history_length)\n",
    "        agent = QLearningAgent(num_agents=num_agents, red_coop_prob=red_coop, blue_coop_prob=blue_coop, history_length=history_length)\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "\n",
    "            for trial in range(num_trials):\n",
    "                actions = agent.choose_action(state)\n",
    "                next_state, rewards, done, _ = env.step(actions)\n",
    "                reward = np.mean(rewards)\n",
    "                agent.update_q_table(state, actions, reward, next_state)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            agent.decay_epsilon()\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "        plot_results_with_qlearning(agent, red_coop, blue_coop)\n",
    "\n",
    "def plot_results_with_qlearning(agent, red_coop, blue_coop):\n",
    "    red_rewards = np.mean(agent.q_table[:, :2 ** (agent.num_agents // 2)], axis=0)\n",
    "    blue_rewards = np.mean(agent.q_table[:, 2 ** (agent.num_agents // 2):], axis=0)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.boxplot([red_rewards, blue_rewards], labels=['Red', 'Blue'])\n",
    "    plt.title(f'Average Rewards (Red Coop: {red_coop}, Blue Coop: {blue_coop})')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_states: 1125899906842624, num_actions: 1024\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m cooperation_strategies \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     (\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m),  \u001b[38;5;66;03m# High cooperation for red, low for blue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m),  \u001b[38;5;66;03m# Equal cooperation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     (\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.7\u001b[39m),  \u001b[38;5;66;03m# Moderately low cooperation for red, moderately high for blue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m \u001b[43mrun_simulation_with_qlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcooperation_strategies\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mrun_simulation_with_qlearning\u001b[0;34m(cooperation_strategies, num_agents, num_episodes, num_trials, history_length)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m red_coop, blue_coop \u001b[38;5;129;01min\u001b[39;00m cooperation_strategies:\n\u001b[1;32m      3\u001b[0m     env \u001b[38;5;241m=\u001b[39m MultiArmedBanditEnv(num_agents\u001b[38;5;241m=\u001b[39mnum_agents, history_length\u001b[38;5;241m=\u001b[39mhistory_length)\n\u001b[0;32m----> 4\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43mQLearningAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mred_coop_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mred_coop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblue_coop_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblue_coop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m      7\u001b[0m         state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/school/EDUC_234/RL_Identity/MABP.py:87\u001b[0m, in \u001b[0;36mQLearningAgent.__init__\u001b[0;34m(self, num_agents, red_coop_prob, blue_coop_prob, history_length, learning_rate, discount_factor, exploration_rate, exploration_decay, min_epsilon)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_states: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_states\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_actions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_actions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
     ]
    }
   ],
   "source": [
    "cooperation_strategies = [\n",
    "    (0.8, 0.2),  # High cooperation for red, low for blue\n",
    "    (0.5, 0.5),  # Equal cooperation\n",
    "    (0.2, 0.8),  # Low cooperation for red, high for blue\n",
    "    (0.7, 0.3),  # Moderately high cooperation for red, moderately low for blue\n",
    "    (0.3, 0.7),  # Moderately low cooperation for red, moderately high for blue\n",
    "]\n",
    "\n",
    "run_simulation_with_qlearning(cooperation_strategies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Identity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
